import os
import sys
from pathlib import Path
from flask import Flask, request, jsonify, send_from_directory, Response, stream_with_context
from flask_cors import CORS
import json
import time
import threading
import queue
import datetime
from config_loader import load_config
try:
    from send2trash import send2trash
except Exception:
    send2trash = None

# Ensure we can import modules
sys.path.append(str(Path(__file__).parent))
import find_similar_files
import find_duplicate_by_hash
import find_history_files
import find_process_files

app = Flask(__name__, static_folder='page')

# Queue for log messages
log_queue = queue.Queue()
# é˜²æ­¢åŒä¸€è¿›ç¨‹å†…å¹¶å‘/é‡å¤å¯åŠ¨åˆ†æä»»åŠ¡ï¼ˆæ‰“åŒ…å UI è¯¯è§¦å‘æ—¶å¾ˆå¸¸è§ï¼‰
analysis_lock = threading.Lock()

def count_files(directory):
    """Recursively count files in directory"""
    count = 0
    try:
        for root, dirs, files in os.walk(directory):
            count += len(files)
    except Exception:
        pass
    return count

def get_folder_size(directory):
    """Get total size of folder in bytes"""
    total_size = 0
    try:
        for root, dirs, files in os.walk(directory):
            for file in files:
                try:
                    file_path = os.path.join(root, file)
                    if os.path.exists(file_path):
                        total_size += os.path.getsize(file_path)
                except Exception:
                    continue
    except Exception:
        pass
    return total_size

@app.route('/')
def index():
    return send_from_directory('page', 'index.html')

@app.route('/<path:path>')
def static_files(path):
    return send_from_directory('page', path)

@app.route('/api/image')
def serve_image():
    """æä¾›æœ¬åœ°å›¾ç‰‡æ–‡ä»¶çš„è®¿é—®ï¼ˆç”¨äºç¼©ç•¥å›¾æ˜¾ç¤ºï¼‰"""
    image_path = request.args.get('path')
    if not image_path:
        return jsonify({"error": "Path parameter required"}), 400
    
    try:
        file_path = Path(image_path)
        if not file_path.exists() or not file_path.is_file():
            return jsonify({"error": "File not found"}), 404
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp', '.tif', '.tiff'}
        if file_path.suffix.lower() not in image_extensions:
            return jsonify({"error": "Not an image file"}), 400
        
        # è¿”å›å›¾ç‰‡æ–‡ä»¶
        return send_from_directory(file_path.parent, file_path.name, mimetype=f'image/{file_path.suffix[1:]}')
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/get_folder_size', methods=['POST'])
def get_folder_size_api():
    """è·å–æ–‡ä»¶å¤¹å¤§å°"""
    try:
        data = request.json
        folder_path = data.get('folder_path')
        
        if not folder_path:
            return jsonify({"error": "Folder path is required"}), 400
        
        path_obj = Path(folder_path)
        if not path_obj.exists() or not path_obj.is_dir():
            return jsonify({"error": f"Invalid directory: {folder_path}"}), 400
        
        # è·å–æ–‡ä»¶å¤¹å¤§å°
        size = get_folder_size(folder_path)
        
        return jsonify({
            "size": size,
            "path": folder_path
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/analyze', methods=['POST'])
def analyze():
    try:
        data = request.json
        print(f"ğŸ“¥ æ”¶åˆ°åˆ†æè¯·æ±‚: {data}")
        
        target_path = data.get('path')
        mode = data.get('mode', 'similar')
        
        if not target_path:
            return jsonify({"error": "Path is required"}), 400
        
        path_obj = Path(target_path)
        if not path_obj.exists() or not path_obj.is_dir():
            return jsonify({"error": f"Invalid directory: {target_path}"}), 400
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    # è‹¥å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œï¼Œç›´æ¥æ‹’ç»ï¼Œé¿å…å…±äº«å…¨å±€çŠ¶æ€å¯¼è‡´ç¬¬äºŒæ¬¡è¿è¡ŒæŠ¥é”™
    if not analysis_lock.acquire(blocking=False):
        return jsonify({"error": "å·²æœ‰åˆ†æä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè¯·ç­‰å¾…å®Œæˆåå†å¼€å§‹æ–°çš„åˆ†æ"}), 409
    
    def run_analysis():
        session_logs = []
        start_time = datetime.datetime.now()
        
        def log_callback(message):
            """Capture log, send to queue, and save to memory"""
            # print(message) # Optional console output
            timestamp = datetime.datetime.now().strftime("%H:%M:%S")
            formatted_msg = f"[{timestamp}] {message}"
            session_logs.append(formatted_msg)
            log_queue.put(message)

        try:
            # è¯»å–é…ç½®ï¼ˆæ”¯æŒæ‰“åŒ…åä» exe ç›®å½•/å·¥ä½œç›®å½•è¯»å– config.jsonï¼‰
            cfg = load_config()

            # æ¸…ç†ä¸Šä¸€è½®é—ç•™æ¶ˆæ¯ï¼Œé¿å…å‰ç«¯æ”¶åˆ°æ··æ‚äº‹ä»¶
            try:
                while True:
                    log_queue.get_nowait()
            except queue.Empty:
                pass
            log_queue.put({"type": "reset"})

            log_callback(f"ğŸš€ å¼€å§‹åˆ†æ: {target_path} (æ¨¡å¼: {mode})")
            
            # 1. Estimate time
            log_callback("â³ æ­£åœ¨æ‰«ææ–‡ä»¶å¹¶è®¡ç®—é¢„ä¼°æ—¶é—´...")
            total_files = count_files(target_path)
            
            # Estimate logicï¼ˆå¯é…ç½®ï¼Œé»˜è®¤åä¿å®ˆï¼Œå°¤å…¶å›¾ç‰‡å¤„ç†ï¼‰
            per_file_time = float(cfg.get('ESTIMATE_PER_FILE_DUPLICATE', 0.15))
            if mode == 'similar':
                per_file_time = float(cfg.get('ESTIMATE_PER_FILE_SIMILAR', 6.0))
            elif mode == 'image':
                per_file_time = float(cfg.get('ESTIMATE_PER_FILE_IMAGE', 5.0))
            elif mode == 'history':
                # å†å²æ–‡ä»¶ï¼šä»…å…ƒä¿¡æ¯åˆ¤å®š + å±‚çº§å‰ªæï¼Œé€šå¸¸éå¸¸å¿«
                per_file_time = float(cfg.get('ESTIMATE_PER_FILE_HISTORY', 0.03))
            elif mode == 'process':
                # è¿‡ç¨‹æ–‡ä»¶ï¼šä¸»è¦åŸºäºæ–‡ä»¶å/å…ƒä¿¡æ¯è§„åˆ™ï¼Œé€šå¸¸å¾ˆå¿«
                per_file_time = float(cfg.get('ESTIMATE_PER_FILE_PROCESS', 0.06))
                
            estimated_seconds = int(total_files * per_file_time)
            # æ·»åŠ åŸºç¡€æ—¶é—´ï¼ˆåˆå§‹åŒ–ã€æ‰«æã€æ¨¡å‹ warmup ç­‰ï¼‰
            estimated_seconds += int(cfg.get('ESTIMATE_BASE_SECONDS', 15))
            if estimated_seconds < 5: estimated_seconds = 5
            
            # Send estimate event
            log_queue.put({
                "type": "estimate", 
                "seconds": estimated_seconds,
                "total_files": total_files
            })
            log_callback(f"ğŸ“„ å…±å‘ç° {total_files} ä¸ªæ–‡ä»¶ï¼Œé¢„è®¡è€—æ—¶ {estimated_seconds} ç§’")

            results = []
            if mode == 'duplicate':
                results = find_duplicate_by_hash.process_directory(path_obj, log_callback=log_callback)
            elif mode == 'history':
                results = find_history_files.process_directory(path_obj, log_callback=log_callback)
            elif mode == 'process':
                results = find_process_files.process_directory(path_obj, log_callback=log_callback)
            elif mode == 'image': # New mode for images
                # Directly call the image logic only (reusing find_similar_files with filter?)
                # Actually find_similar_files.process_directory now does BOTH if images exist.
                # To be cleaner, we might want to tell it to ONLY do images.
                # But for now, let's reuse process_directory and rely on its internal logic
                # Maybe we can add a 'mode' param to process_directory later.
                # For now, let's just use it as is, it handles images.
                results = find_similar_files.process_directory(path_obj, log_callback=log_callback)
            else:
                # 'similar' mode
                results = find_similar_files.process_directory(path_obj, log_callback=log_callback)
            
            if results is None:
                results = []
            
            log_callback(f"âœ… åˆ†æå®Œæˆ. æ‰¾åˆ° {len(results)} ç»„ç»“æœã€‚")
            log_queue.put({"type": "result", "data": results, "mode": mode})
            
            # 3. Save Log to Markdown
            log_dir = path_obj / "logs"
            log_dir.mkdir(exist_ok=True)
            log_file = log_dir / f"analysis_log_{start_time.strftime('%Y%m%d_%H%M%S')}.md"
            
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"# åˆ†ææ—¥å¿— - {mode}\n\n")
                f.write(f"**æ—¶é—´**: {start_time}\n")
                f.write(f"**è·¯å¾„**: {target_path}\n")
                f.write(f"**æ–‡ä»¶æ€»æ•°**: {total_files}\n")
                f.write(f"**å‘ç°ç»“æœ**: {len(results)} ç»„\n\n")
                f.write("## è¯¦ç»†æ—¥å¿—\n\n")
                for line in session_logs:
                    f.write(f"- {line}\n")
            
            log_callback(f"ğŸ“ æ—¥å¿—å·²ä¿å­˜è‡³: {log_file.name}")

        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            log_callback(f"âŒ é”™è¯¯: {str(e)}")
            log_callback(f"è¯¦ç»†é”™è¯¯: {error_trace}")
            log_queue.put({"type": "error", "message": str(e)})
        finally:
            # é‡Šæ”¾é”ï¼Œå…è®¸ä¸‹ä¸€æ¬¡åˆ†æ
            try:
                analysis_lock.release()
            except Exception:
                pass

    thread = threading.Thread(target=run_analysis)
    thread.start()
    
    return jsonify({"status": "started", "message": "Analysis started"})


@app.route('/api/delete', methods=['POST'])
def delete_files():
    """å°†ç”¨æˆ·å‹¾é€‰çš„æ–‡ä»¶ç§»åˆ°å›æ”¶ç«™/åºŸçº¸ç¯“ï¼ˆå‰ç«¯å‹¾é€‰ â†’ åç«¯æ‰§è¡Œï¼‰"""
    try:
        data = request.json or {}
        root = data.get('root')
        paths = data.get('paths') or []

        if not root:
            return jsonify({"error": "root is required"}), 400
        if not isinstance(paths, list) or not paths:
            return jsonify({"error": "paths must be a non-empty list"}), 400

        root_path = Path(root).resolve()
        if not root_path.exists() or not root_path.is_dir():
            return jsonify({"error": f"Invalid root directory: {root}"}), 400

        deleted = []
        failed = []

        for p in paths:
            try:
                fp = Path(p).resolve()
                # å®‰å…¨æ ¡éªŒï¼šå¿…é¡»åœ¨ root ç›®å½•å†…
                if root_path not in fp.parents and fp != root_path:
                    failed.append({"path": p, "error": "path is outside root"})
                    continue
                if not fp.exists() or not fp.is_file():
                    failed.append({"path": p, "error": "file not found"})
                    continue
                if send2trash is None:
                    failed.append({"path": p, "error": "send2trash not available"})
                    continue
                # ç§»åˆ°å›æ”¶ç«™/åºŸçº¸ç¯“ï¼ˆè·¨å¹³å°ï¼‰
                send2trash(str(fp))
                deleted.append(p)
            except Exception as e:
                failed.append({"path": p, "error": str(e)})

        return jsonify({"deleted": deleted, "failed": failed}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Custom JSON encoder to handle datetime and other objects
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime.date, datetime.datetime)):
            return obj.isoformat()
        if isinstance(obj, Path):
            return str(obj)
        return super().default(obj)

@app.route('/api/stream')
def stream():
    def event_stream():
        while True:
            try:
                message = log_queue.get(timeout=30)
                if isinstance(message, dict):
                    # Use custom encoder
                    yield f"data: {json.dumps(message, cls=CustomJSONEncoder)}\n\n"
                else:
                    yield f"data: {json.dumps({'type': 'log', 'message': message}, cls=CustomJSONEncoder)}\n\n"
            except queue.Empty:
                yield f"data: {json.dumps({'type': 'ping'})}\n\n"
    
    return Response(stream_with_context(event_stream()), mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(port=5000, debug=True)
