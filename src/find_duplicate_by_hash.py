import os
import hashlib
from collections import defaultdict
from datetime import datetime


def compute_md5(file_path, block_size=8192):
    """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œï¼ˆåˆ†å—è¯»å–ä»¥èŠ‚çœå†…å­˜ï¼‰"""
    md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(block_size), b""):
                md5.update(chunk)
        return md5.hexdigest()
    except Exception:
        return None


def get_file_info(file_path):
    """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ï¼ˆå«åˆ›å»ºæ—¶é—´ï¼‰"""
    try:
        stat = os.stat(file_path)
        return {
            "path": file_path,
            "size": stat.st_size,
            "mtime": stat.st_mtime,
            "atime": stat.st_atime,
            "ctime": stat.st_ctime,
        }
    except Exception:
        return None


def find_duplicate_by_hash(folder_path):
    """æ‰«ææ–‡ä»¶å¤¹å¹¶æ‰¾å‡ºå“ˆå¸Œå€¼ç›¸åŒï¼ˆå†…å®¹å®Œå…¨ç›¸åŒï¼‰çš„æ–‡ä»¶ã€‚"""
    file_infos = []
    for root, _, files in os.walk(folder_path):
        for name in files:
            full_path = os.path.join(root, name)
            info = get_file_info(full_path)
            if info:
                file_infos.append(info)

    size_groups = defaultdict(list)
    for info in file_infos:
        size_groups[info["size"]].append(info)

    hash_dict = defaultdict(list)
    for size, group in size_groups.items():
        if len(group) < 2:
            continue
        for item in group:
            md5 = compute_md5(item["path"])
            if md5:
                hash_dict[md5].append(item)

    duplicates = [group for group in hash_dict.values() if len(group) > 1]
    duplicates.sort(key=lambda g: g[0]["size"], reverse=True)
    return duplicates


def suggest_cleanup_for_duplicates(
    duplicates,
    inactive_days=180,
    active_days=30,
    large_file_mb=20
):
    """
    ç”Ÿæˆæ¸…ç†å»ºè®®ã€‚
    - ç»„çº§â€œéœ€æ¸…ç†â€ï¼šä½“ç§¯â‰¥large_file_mb ä¸”æœ€è¿‘ä¿®æ”¹æ—¶é—´è·ä»Š>inactive_days
    - ç»„å†…ä¿ç•™ï¼šä¼˜å…ˆæœ€è¿‘è®¿é—®(atime)ï¼›è‹¥ç›¸åŒç”¨åˆ›å»ºæ—¶é—´(ctime)ä½œtiebreakerã€‚
    """
    now = datetime.now()
    results = []

    for idx, group in enumerate(duplicates, start=1):
        for f in group:
            f["size_mb"] = round(f["size"] / (1024 * 1024), 2)
            f["last_modify"] = datetime.fromtimestamp(f["mtime"])
            f["last_access"] = datetime.fromtimestamp(f["atime"])
            f["ctime_dt"] = datetime.fromtimestamp(f.get("ctime", f["mtime"]))
            f["days_since_access"] = (now - f["last_access"]).days
            f["days_since_modify"] = (now - f["last_modify"]).days
            f["is_active"] = f["days_since_access"] <= active_days

        group_size_mb = round(group[0]["size"] / (1024 * 1024), 2)
        newest_modify_dt = max(f["last_modify"] for f in group)
        days_since_newest_modify = (now - newest_modify_dt).days

        group_needs_cleanup = (group_size_mb >= large_file_mb) and (days_since_newest_modify > inactive_days)

        keep = max(group, key=lambda x: (x["last_access"].timestamp(), x["ctime_dt"].timestamp(), x["path"]))

        for f in group:
            if f is keep:
                f["suggestion"] = "âœ… ä¿ç•™"
            else:
                reasons = []
                if not f["is_active"]:
                    reasons.append(f"æœªè®¿é—® {f['days_since_access']} å¤©")
                if f["size_mb"] >= large_file_mb:
                    reasons.append(f"ä½“ç§¯â‰¥{large_file_mb}MB")
                reason_text = ", ".join(reasons) if reasons else "å†—ä½™å‰¯æœ¬"
                f["suggestion"] = f"ğŸ—‘ å¼ºçƒˆåˆ é™¤ï¼ˆ{reason_text}ï¼‰" if group_needs_cleanup else f"ğŸ—‘ åˆ é™¤ï¼ˆ{reason_text}ï¼‰"

        # æ ¼å¼åŒ–æ–‡ä»¶å¯¹è±¡ä»¥åŒ¹é…å‰ç«¯æœŸæœ›çš„æ ¼å¼
        formatted_files = []
        for f in group:
            formatted_files.append({
                "path": str(f["path"]),
                "name": os.path.basename(str(f["path"])),
                "size": f["size"],
                "mtime": f["mtime"],
                "suggestion": f["suggestion"]
            })
        
        results.append({
            "type": "duplicate",
            "group_id": idx,
            "fileSize": group_size_mb,
            "file_size_mb": group_size_mb,  # ä¿ç•™å…¼å®¹æ€§
            "last_modify": newest_modify_dt.strftime("%Y-%m-%d"),
            "days_since_modify": days_since_newest_modify,
            "need_cleanup": group_needs_cleanup,
            "files": formatted_files,
            "analysis": f"è¿™äº›æ–‡ä»¶å†…å®¹ç›¸åŒï¼Œæœ€è¿‘ä¿®æ”¹æ—¶é—´ï¼š{newest_modify_dt.strftime('%Y-%m-%d')}"
        })

    return results


def process_directory(target_dir, log_callback=None):
    """
    ä¸»å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶ç›®æ ‡ç›®å½•å¹¶è¿”å›ç»“æœã€‚
    """
    def log(msg):
        if log_callback:
            log_callback(msg)
        else:
            print(msg)

    folder_path = str(target_dir)
    log(f"ğŸš€ å¼€å§‹æ‰«æé‡å¤æ–‡ä»¶: {folder_path}")
    
    try:
        duplicates = find_duplicate_by_hash(folder_path)
        if not duplicates:
            log("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
            return []
            
        log(f"ğŸ” å‘ç° {len(duplicates)} ç»„é‡å¤æ–‡ä»¶ï¼Œæ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        results = suggest_cleanup_for_duplicates(duplicates, inactive_days=180, active_days=30, large_file_mb=20)
        
        # ç®€å•ç»Ÿè®¡
        total_waste = sum(g['file_size_mb'] * (len(g['files']) - 1) for g in results)
        log(f"ğŸ“Š é¢„è®¡å¯é‡Šæ”¾ç©ºé—´: {total_waste:.2f} MB")
        
        return results
        
    except Exception as e:
        log(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []
